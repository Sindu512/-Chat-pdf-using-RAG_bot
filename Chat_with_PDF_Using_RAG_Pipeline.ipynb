{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sindu512/-Chat-pdf-using-RAG_bot/blob/main/Chat_with_PDF_Using_RAG_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BipBLkRMIwsX",
        "outputId": "cbeee813-6dc7-4667-9df8-436f44c962b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf2 sentence-transformers faiss-cpu --quiet"
      ],
      "metadata": {
        "id": "ZQTOYj3UIzJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pdfplumber\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "1zjfx7sQJBFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    extracted_text = \"\"\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            extracted_text += page.extract_text() + \"\\n\"\n",
        "    return extracted_text"
      ],
      "metadata": {
        "id": "vjLLLzu3JYzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split text into chunks\n",
        "def split_into_chunks(text, chunk_size=100):\n",
        "    sentences = text.split(\". \")\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk.split()) + len(sentence.split()) <= chunk_size:\n",
        "            current_chunk += sentence + \". \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \". \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "DDHna7BZJeCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create embeddings for the chunks\n",
        "def create_embeddings(chunks, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(chunks)\n",
        "    return embeddings, model\n"
      ],
      "metadata": {
        "id": "WMv6c6L8JlB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Store embeddings in FAISS\n",
        "\n",
        "def store_embeddings(embeddings):\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "    return index"
      ],
      "metadata": {
        "id": "oKUnFDIXJs65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Search the most relevant chunks\n",
        "def search_query(query, index, chunks, model):\n",
        "    query_embedding = model.encode([query])\n",
        "    distances, indices = index.search(query_embedding, k=1)\n",
        "\n",
        "    results = []\n",
        "    for i in indices[0]:\n",
        "        results.append(chunks[i])\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "9gQxBNazJxsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Use a mock LLM to generate a response\n",
        "def generate_response(query, results):\n",
        "    response = \"Here are the relevant results for your query: \\n\\n\"\n",
        "    for result in results:\n",
        "        response += f\"- {result}\\n\\n\"\n",
        "    return response"
      ],
      "metadata": {
        "id": "T-koDNulJ3gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Script\n",
        "def main():\n",
        "    # 1. Path to the PDF\n",
        "    pdf_path = \"/content/drive/MyDrive/Task1.pdf\"  # Replace with your PDF path\n",
        "\n",
        "    # 2. Extract and preprocess text\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "    chunks = split_into_chunks(text)\n",
        "\n",
        "    # 3. Create embeddings\n",
        "    embeddings, model = create_embeddings(chunks)\n",
        "\n",
        "    # 4. Store embeddings in FAISS index\n",
        "    index = store_embeddings(np.array(embeddings))\n",
        "\n",
        "    # 5. Query the system\n",
        "    query = \"can you give U.S. Bureau of Labor Statistics?\"  # Replace with your query\n",
        "    results = search_query(query, index, chunks, model)\n",
        "\n",
        "    # 6. Generate and print response\n",
        "    response = generate_response(query, results)\n",
        "    print(response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRiJeWFyKAKn",
        "outputId": "83a66956-5b72-44d9-c3a9-3b9a29813748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are the relevant results for your query: \n",
            "\n",
            "- Bureau of Labor Statistics\n",
            "Year 2010 2011 2012 2013 2014 2015\n",
            "All Industries 26093515 27535971 28663246 29601191 30895407 31397023\n",
            "Manufacturing 4992521 5581942 5841608 5953299 6047477 5829554\n",
            "Finance,\n",
            "Insurance, Real\n",
            "4522451 4618678 4797313 5031881 5339678 5597018\n",
            "Estate, Rental,\n",
            "Leasing\n",
            "Arts,\n",
            "Entertainment,\n",
            "Recreation, 964032 1015238 1076249 1120496 1189646 1283813\n",
            "Accommodation,\n",
            "and Food Service\n",
            "Other 15614511 16320113 16948076 17495515 18318606 18686638\n",
            "â€¢ The chart below is called a pie chart.\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}